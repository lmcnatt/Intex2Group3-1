{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('intex2.db')\n",
    "cursor = conn.cursor()\n",
    "read_sql = \"SELECT P.ProductId, P.Name, P.Description, CL.Rating, O.CustomerId FROM Products P\\\n",
    "    join CartLine CL on CL.ProductId = P.ProductId join Orders O on O.OrderId = CL.OrderId\"\n",
    "df = pd.read_sql_query(read_sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_ratings(df, count_column, min=2, messages=True):\n",
    "  value_counts = df[count_column].value_counts()\n",
    "  keep_list = value_counts[value_counts >= min]\n",
    "  df = df.loc[df[count_column].isin(keep_list.index)]\n",
    "\n",
    "  if messages: print(df[count_column].value_counts())\n",
    "\n",
    "  return df\n",
    "df.drop_duplicates(subset=['CustomerId', 'ProductId'], keep='first', inplace=True)\n",
    "#We are keeping the first because ratings should be based on intial difficulty or enjoyment\n",
    "df_collab = min_ratings(df, 'ProductId', min=100, messages=False) \n",
    "#We set this value low so only new products use collaborative since content isnt very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this does the collaborative filtering\n",
    "df_products = df.groupby('ProductId').agg({'Name':'max',\n",
    "                                       'Description':'max',\n",
    "                                       'Rating':'count'})\n",
    "def create_matrix(df, user, item, rating):\n",
    "  import numpy as np\n",
    "  from scipy.sparse import csr_matrix\n",
    "\n",
    "  U = df[user].nunique()  # Number of users for the matrix\n",
    "  I = df[item].nunique()  # Number of items for the matrix\n",
    "\n",
    "  # Map user and item IDs to matrix indices\n",
    "  user_mapper = dict(zip(np.unique(df[user]), list(range(U))))\n",
    "  item_mapper = dict(zip(np.unique(df[item]), list(range(I))))\n",
    "\n",
    "  # Map matrix indices back to IDs\n",
    "  user_inv_mapper = dict(zip(list(range(U)), np.unique(df[user])))\n",
    "  item_inv_mapper = dict(zip(list(range(I)), np.unique(df[item])))\n",
    "\n",
    "  # Create a list of index values for the csr_matrix for users and movies\n",
    "  user_index = [user_mapper[i] for i in df[user]]\n",
    "  item_index = [item_mapper[i] for i in df[item]]\n",
    "\n",
    "  # Build the final matrix which will look like: (itemId, userId) rating\n",
    "  X = csr_matrix((df[rating], (item_index, user_index)), shape=(I, U))\n",
    "\n",
    "  return X, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper\n",
    "\n",
    "def collab_recommend(itemId, X, item_mapper, item_inv_mapper, k, metric='cosine', messages=True):\n",
    "  from sklearn.neighbors import NearestNeighbors\n",
    "  import numpy as np\n",
    "\n",
    "  rec_ids = []                # Make a list for the recommended item IDs we'll get later\n",
    "  item = item_mapper[itemId]  # Get the index of the item ID passed into the function\n",
    "  item_vector = X[item]       # Get the vector of user ratings for the item ID passed into the function\n",
    "\n",
    "  # Fit the clustering algorithm based on the user-item matrix X\n",
    "  knn = NearestNeighbors(n_neighbors=k+1, algorithm=\"brute\", metric=metric).fit(X)\n",
    "\n",
    "  # Call the trained knn cluster model to return the nearest neighbors of the item_vector passed in\n",
    "  rec = knn.kneighbors(item_vector.reshape(1,-1), return_distance=True)\n",
    "  rec_indeces = rec[1][0]     # Parse out the list of indeces of the recommended items\n",
    "  rec_distances = rec[0][0]   # Parse out the recommendation strength calculated as the distance from the cluster center\n",
    "  rec_distances = np.delete(rec_distances, 0) # Drop the first number in the list because it is the distance of itemId from itself\n",
    "\n",
    "  # We need to replace the recommended item indeces with their original item IDs\n",
    "  for i in range(1, knn.n_neighbors): # n_neighbors is the number of neighbors to return\n",
    "    rec_ids.append(item_inv_mapper[rec_indeces[i]])\n",
    "\n",
    "  # It may help to see what this is. The distance list is first and the recommended item indeces are second\n",
    "  if messages:\n",
    "    print(f'List of recommended item indeces:\\n{rec_indeces}\\n')\n",
    "    print(f'List of recommended item IDs:\\n{rec_ids}\\n')\n",
    "    print(f'List of recommended item similarity to selected item:\\n{rec_distances}\\n')\n",
    "\n",
    "  # Return two lists: the original item IDs of the recommendations and their similarity scores\n",
    "  return rec_ids, rec_distances\n",
    "\n",
    "X, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper = create_matrix(df_collab, 'CustomerId', 'ProductId', 'Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This starts Content filtering\n",
    "def tfidf_matrix(df, similarity_col):\n",
    "  import numpy as np\n",
    "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "  # Create a TfidfVectorizer and Remove stopwords\n",
    "  tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "  # Fit and transform the data to a tfidf matrix\n",
    "  tfidf_matrix = tfidf.fit_transform(df[similarity_col])\n",
    "\n",
    "  # Build the final matrix which will look like: (movieId, userId) rating\n",
    "  cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "  return cosine_sim\n",
    "def content_recommend(item_id, sim_matrix, n=10, messages=True):\n",
    "  if item_id > sim_matrix.shape[0]:  # Add some error checking for robustness\n",
    "    print(f\"Item {item_id} is not in the similarity matrix you provided with shape: {sim_matrix.shape}\")\n",
    "    return\n",
    "\n",
    "  # Get the pairwise similarity scores of all movies with that movie\n",
    "  sim_scores = list(enumerate(sim_matrix[item_id]))\n",
    "\n",
    "  # Sort the items based on the similarity scores\n",
    "  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Get the scores of the n most similar items; start at 1 so that it skips itself\n",
    "  top_similar = sim_scores[1:n+1]\n",
    "\n",
    "  # Put the recommended item indices and similarity scores together in a dictionary using comprehension\n",
    "  rec_dict = {i[0]:i[1] for i in top_similar}\n",
    "\n",
    "  if messages:\n",
    "    print(f\"The top recommended item IDs are: {list(rec_dict.keys())}\")\n",
    "    print(f\"Their similarity scores are:\\t  {list(rec_dict.values())}\")\n",
    "\n",
    "  # Return the top n most similar items\n",
    "  return rec_dict\n",
    "\n",
    "df_products.reset_index(inplace=True)\n",
    "df_products['similarity'] = df_products['Name'] + \" \" + df_products['Description']\n",
    "sim_matrix = tfidf_matrix(df_products, 'similarity')\n",
    "df_products.drop(columns=['similarity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This brings them together\n",
    "threshold = 100\n",
    "\n",
    "df_recommendations = pd.DataFrame(columns=['RecId', 'Rec1', 'Rec2', 'Rec3', 'Rec4', 'Rec5',\n",
    "                                           'Rec6', 'Rec7', 'Rec8', 'Rec9', 'Rec10'])\n",
    "\n",
    "for row in df_products.itertuples():\n",
    "  if row.Rating >= threshold:\n",
    "    rec_ids, rec_distances = collab_recommend(row.ProductId, X, item_mapper, item_inv_mapper, k=10, messages=False)\n",
    "  else:\n",
    "    recommend_dict = content_recommend(row[0], sim_matrix, n=10, messages=False)\n",
    "    rec_ids = list(recommend_dict.keys())\n",
    "    rec_distances = list(recommend_dict.values())\n",
    "\n",
    "  df_recommendations.loc[row[0]] = [row.ProductId, rec_ids[0], rec_ids[1], rec_ids[2], rec_ids[3], rec_ids[4], rec_ids[5], rec_ids[6], rec_ids[7], rec_ids[8], rec_ids[9]]\n",
    "\n",
    "df_recommendations.to_sql(name=\"Recommendations\", con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.read_csv(r'\"C:\\Users\\gooch\\Downloads\\INTEX W24 Dataset.xlsx - Products.csv\"')\n",
    "df_products.to_sql(name=\"Products\", con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# cursor.close()\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
