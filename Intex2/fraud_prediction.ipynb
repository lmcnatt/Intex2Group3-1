{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gooch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\gooch\\AppData\\Local\\Temp\\ipykernel_19412\\2054174463.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[label] = y\n",
      "C:\\Users\\gooch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 188\u001b[0m\n\u001b[0;32m    186\u001b[0m model \u001b[38;5;241m=\u001b[39m fit_cv_model(new_5_percent_df, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, label\u001b[38;5;241m=\u001b[39mlabel, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    187\u001b[0m df_reduced \u001b[38;5;241m=\u001b[39m select_features(new_5_percent_df\u001b[38;5;241m.\u001b[39mcopy(), label\u001b[38;5;241m=\u001b[39mlabel, model\u001b[38;5;241m=\u001b[39mmodel, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 188\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfit_cv_model_expanded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m X, y \u001b[38;5;241m=\u001b[39m Xandy(df_reduced, label)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[11], line 158\u001b[0m, in \u001b[0;36mfit_cv_model_expanded\u001b[1;34m(df, label, k, r, repeat, random_state)\u001b[0m\n\u001b[0;32m    156\u001b[0m fit[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cross_val_score(model_knn, X, y, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    157\u001b[0m fit[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cross_val_score(model_xgb, X, y, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 158\u001b[0m fit[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeural Network\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Add the model to another dict; make sure the keys have the same names as the list above\u001b[39;00m\n\u001b[0;32m    161\u001b[0m models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_logistic\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:714\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    712\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 714\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:425\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 425\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import onnx # not used but can be used to inspect and manipulate saved onnx model\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "conn = sqlite3.connect('intex2.db')\n",
    "cursor = conn.cursor()\n",
    "read_sql = \"SELECT * from Orders\"\n",
    "df = pd.read_sql_query(read_sql, conn)\n",
    "df\n",
    "\n",
    "\n",
    "#This has all the functions\n",
    "def bin_categories(df, features=[], cutoff=0.05, replace_with='Other', messages=False):\n",
    "\n",
    "  import pandas as pd\n",
    "\n",
    "  if len(features) == 0: features = df.columns\n",
    "\n",
    "  for feat in features:\n",
    "    if feat in df.columns:\n",
    "      if not pd.api.types.is_numeric_dtype(df[feat]):\n",
    "        other_list = df[feat].value_counts()[df[feat].value_counts() / df.shape[0] < cutoff].index\n",
    "        df.loc[df[feat].isin(other_list), feat] = replace_with\n",
    "        if messages: print(f'{feat} has been binned by setting {other_list} to {replace_with}')\n",
    "    else:\n",
    "      if messages: print(f'{feat} not found in the DataFrame provided. No binning performed')\n",
    "\n",
    "  return df\n",
    "\n",
    "def Xandy(df, label):\n",
    "    import pandas as pd\n",
    "    y = df[label]\n",
    "    X = df.drop(columns=[label])\n",
    "    return X, y\n",
    "\n",
    "def dummy_code(X):\n",
    "    import pandas as pd\n",
    "    X = pd.get_dummies(X.copy(), drop_first=True)\n",
    "    return X\n",
    "\n",
    "def missing_data(df, label, row_thresh = 0.7, col_thresh = 0.9, random=False, random_state=3):\n",
    "    import pandas as pd\n",
    "    df.dropna(axis='rows', subset=[label], inplace=True)\n",
    "    df.dropna(axis='columns', thresh=1, inplace=True)\n",
    "    df.dropna(axis='rows', thresh=1, inplace=True)\n",
    "    df.dropna(axis='columns', thresh=round(df.shape[0] * row_thresh), inplace=True)\n",
    "    df.dropna(axis='rows', thresh=round(df.shape[1] * col_thresh), inplace=True)\n",
    "    #impute values\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer, KNNImputer\n",
    "        X, y = Xandy(df, label)\n",
    "        X = dummy_code(X.copy())\n",
    "        if random: random_state = 0\n",
    "        imp = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "        X = pd.DataFrame(imp.fit_transform(X), columns = X.columns, index = X.index)\n",
    "        df = X.merge(y, left_index=True, right_index=True)\n",
    "    else:\n",
    "        X, y = Xandy(df, label)\n",
    "        X = dummy_code(X.copy())\n",
    "        df = X.merge(y, left_index=True, right_index=True)\n",
    "    return df\n",
    "\n",
    "def fit_cv_model(df, label, k=5, repeat=True, random=False, random_state=3, messages=False):\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    X, y = Xandy(df, label)\n",
    "    X = dummy_code(X.copy())\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=5)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k)\n",
    "    if random==True: random_state = 0\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "    model_rfc = RandomForestClassifier(random_state=random_state)\n",
    "    model_ridge = RidgeClassifier(random_state=random_state)\n",
    "    model_gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "    model_log = LogisticRegression(random_state=random_state, max_iter=10000)\n",
    "    scores_rfc = cross_val_score(model_rfc, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_ridge = cross_val_score(model_ridge, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_gbc = cross_val_score(model_gbc, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_log = cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    if messages == True:\n",
    "        print(f'Accuracy (RandomForest):\\t{np.mean(scores_rfc)}')\n",
    "        print(f'Accuracy (Ridge):\\t{np.mean(scores_ridge)}')\n",
    "        print(f'Accuracy (GradientBoosting):\\t{np.mean(scores_gbc)}')\n",
    "        print(f'Accuracy (Log):\\t{np.mean(scores_log)}')\n",
    "    scores = {np.mean(scores_rfc):model_rfc, \n",
    "            np.mean(scores_gbc):model_gbc, \n",
    "            np.mean(scores_ridge):model_ridge, \n",
    "            np.mean(scores_log):model_log }    \n",
    "    return scores[max(scores.keys())].fit(X, y)\n",
    "\n",
    "def select_features(df, label, model, messages=True):\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "  import pandas as pd\n",
    "\n",
    "  y = df[label]\n",
    "  X = df.drop(columns=[label])\n",
    "  X = pd.get_dummies(X.copy(), drop_first=True)\n",
    "\n",
    "  sel = SelectFromModel(model, prefit=True)\n",
    "  sel.transform(X)\n",
    "\n",
    "  columns = list(X.columns[sel.get_support()])\n",
    "  new_df = X[columns]\n",
    "  new_df[label] = y\n",
    "  return new_df\n",
    "\n",
    "def fit_cv_model_expanded(df, label, k=10, r=5, repeat=True, random_state=0):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Assuming Xandy is a function to split features (X) and target (y)\n",
    "    X, y = Xandy(df, label)\n",
    "\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    fit = {}    # Use this to store each of the fit metrics\n",
    "    models = {} # Use this to store each of the models\n",
    "\n",
    "    # Create the model objects\n",
    "    model_logistic = LogisticRegression(random_state=random_state)\n",
    "    model_rf = RandomForestClassifier(random_state=random_state)\n",
    "    model_gb = GradientBoostingClassifier(random_state=random_state)\n",
    "    model_ab = AdaBoostClassifier(n_estimators=100, random_state=random_state)\n",
    "    model_svc = SVC(random_state=random_state)\n",
    "    model_lsvc = LinearSVC(random_state=random_state)\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    model_xgb = XGBClassifier(n_estimators=1000, max_depth=7, learning_rate=0.1, subsample=0.7, colsample_bytree=0.8, random_state=random_state)\n",
    "    model_mlp = MLPClassifier(max_iter=1000, random_state=random_state)\n",
    "\n",
    "    # Fit a cross-validated accuracy score and add it to the dict\n",
    "    fit['Logistic Regression'] = np.mean(cross_val_score(model_logistic, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Random Forest'] = np.mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Gradient Boosting'] = np.mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['AdaBoost'] = np.mean(cross_val_score(model_ab, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['SVM'] = np.mean(cross_val_score(model_svc, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Linear SVM'] = np.mean(cross_val_score(model_lsvc, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['KNN'] = np.mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['XGBoost'] = np.mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Neural Network'] = np.mean(cross_val_score(model_mlp, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "    # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "    models['Logistic Regression'] = model_logistic\n",
    "    models['Random Forest'] = model_rf\n",
    "    models['Gradient Boosting'] = model_gb\n",
    "    models['AdaBoost'] = model_ab\n",
    "    models['SVM'] = model_svc\n",
    "    models['Linear SVM'] = model_lsvc\n",
    "    models['KNN'] = model_knn\n",
    "    models['XGBoost'] = model_xgb\n",
    "    models['Neural Network'] = model_mlp\n",
    "\n",
    "    # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "    df_fit = pd.DataFrame({'Accuracy': fit})\n",
    "    df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "    print(df_fit)\n",
    "    best_model = df_fit.index[0]\n",
    "    \n",
    "\n",
    "    return models[best_model].fit(X, y)\n",
    "\n",
    "\n",
    "df = df.sample(n=100000)\n",
    "#We are not limiting down to CVC and Online only because if there is other types of transactions those need to be known\n",
    "new_5_percent_df = bin_categories(df.copy(), cutoff=0.05)\n",
    "label = 'Fraud'\n",
    "new_5_percent_df = missing_data(new_5_percent_df, label)\n",
    "model = fit_cv_model(new_5_percent_df, k=5, label=label, messages=False)\n",
    "df_reduced = select_features(new_5_percent_df.copy(), label=label, model=model, messages=False)\n",
    "model = fit_cv_model_expanded(df_reduced, k=5, label=label)\n",
    "X, y = Xandy(df_reduced, label)\n",
    "print(X.shape)\n",
    "print(df_reduced.shape)\n",
    "for col in X:\n",
    "    print(f'{col}')\n",
    "for col in df_reduced:\n",
    "    print(f'{col}')\n",
    "initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "with open(\"fraud_onnx_model.onnx\", \"wb\")as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
