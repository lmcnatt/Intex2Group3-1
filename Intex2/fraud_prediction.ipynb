{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fraud Prediction**\n",
    "This program runs the fraud prediction pipeline. Most of the code is rather straightforward and similar to what was done in class, so any differences are commented on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gooch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\gooch\\AppData\\Local\\Temp\\ipykernel_6300\\2054174463.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[label] = y\n",
      "C:\\Users\\gooch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy\n",
      "AdaBoost             0.963430\n",
      "Gradient Boosting    0.963430\n",
      "XGBoost              0.963364\n",
      "Neural Network       0.962022\n",
      "Random Forest        0.960134\n",
      "KNN                  0.958704\n",
      "Logistic Regression  0.950424\n",
      "SVM                  0.928050\n",
      "Linear SVM           0.879212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gooch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n",
      "(100000, 4)\n",
      "Time\n",
      "Amount\n",
      "CountryOfTransaction_United Kingdom\n",
      "Time\n",
      "Amount\n",
      "CountryOfTransaction_United Kingdom\n",
      "Fraud\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "server = \"subdomain.domain.com\" # Change these values to your SQL Server\n",
    "user = \"dbusername\"\n",
    "pw = \"dbusernames_password\"\n",
    "database = \"db_name\"\n",
    "driver= '{ODBC Driver 13 for SQL Server}'\n",
    "conn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+user+';PWD='+ pw)\n",
    "import onnx # not used but can be used to inspect and manipulate saved onnx model\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cursor = conn.cursor()\n",
    "read_sql = \"SELECT * from Orders\"\n",
    "df = pd.read_sql_query(read_sql, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate(df):\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df_output = pd.DataFrame(columns = \n",
    "        ['type', 'missing', 'unique', 'min', 'q1', 'median', 'q3', 'max', 'mode', 'mean', 'std', 'skew', 'kurt']) \n",
    "    for col in df: \n",
    "        missing = df[col].isna().sum() \n",
    "        unique = df[col].nunique() \n",
    "        mode = df[col].mode()[0] \n",
    "        if pd.api.types.is_numeric_dtype(df[col]): \n",
    "            mean = df[col].mean() \n",
    "            min = df[col].min()\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            median = df[col].median()\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            max = df[col].max()\n",
    "            std = df[col].std()\n",
    "            skew = df[col].skew()\n",
    "            kurt = df[col].kurt()\n",
    "            df_output.loc[col] = ['numeric', missing, unique, min, q1, median, q3, max, mode, round(mean, 2), std, skew, kurt]\n",
    "            sns.histplot(data=df, x=col)\n",
    "            plt.show()\n",
    "        else:\n",
    "            df_output.loc[col] = ['categorical', missing, unique, '-', '-', '-', '-', '-', mode, '-', '-', '-', '-'] \n",
    "            sns.countplot(data=df, x=col) \n",
    "            plt.show()\n",
    "    return df_output\n",
    "\n",
    "univariate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This has all the functions\n",
    "def bin_categories(df, features=[], cutoff=0.05, replace_with='Other', messages=False):\n",
    "\n",
    "  import pandas as pd\n",
    "\n",
    "  if len(features) == 0: features = df.columns\n",
    "\n",
    "  for feat in features:\n",
    "    if feat in df.columns:\n",
    "      if not pd.api.types.is_numeric_dtype(df[feat]):\n",
    "        other_list = df[feat].value_counts()[df[feat].value_counts() / df.shape[0] < cutoff].index\n",
    "        df.loc[df[feat].isin(other_list), feat] = replace_with\n",
    "        if messages: print(f'{feat} has been binned by setting {other_list} to {replace_with}')\n",
    "    else:\n",
    "      if messages: print(f'{feat} not found in the DataFrame provided. No binning performed')\n",
    "\n",
    "  return df\n",
    "\n",
    "def Xandy(df, label):\n",
    "    import pandas as pd\n",
    "    y = df[label]\n",
    "    X = df.drop(columns=[label])\n",
    "    return X, y\n",
    "\n",
    "def dummy_code(X):\n",
    "    import pandas as pd\n",
    "    X = pd.get_dummies(X.copy(), drop_first=True)\n",
    "    return X\n",
    "\n",
    "def missing_data(df, label, row_thresh = 0.7, col_thresh = 0.9, random=False, random_state=3):\n",
    "    import pandas as pd\n",
    "    df.dropna(axis='rows', subset=[label], inplace=True)\n",
    "    df.dropna(axis='columns', thresh=1, inplace=True)\n",
    "    df.dropna(axis='rows', thresh=1, inplace=True)\n",
    "    df.dropna(axis='columns', thresh=round(df.shape[0] * row_thresh), inplace=True)\n",
    "    df.dropna(axis='rows', thresh=round(df.shape[1] * col_thresh), inplace=True)\n",
    "    #impute values\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer, KNNImputer\n",
    "        X, y = Xandy(df, label)\n",
    "        X = dummy_code(X.copy())\n",
    "        if random: random_state = 0\n",
    "        imp = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "        X = pd.DataFrame(imp.fit_transform(X), columns = X.columns, index = X.index)\n",
    "        df = X.merge(y, left_index=True, right_index=True)\n",
    "    else:\n",
    "        X, y = Xandy(df, label)\n",
    "        X = dummy_code(X.copy())\n",
    "        df = X.merge(y, left_index=True, right_index=True)\n",
    "    return df\n",
    "\n",
    "def fit_cv_model(df, label, k=5, repeat=True, random=False, random_state=3, messages=False):\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    X, y = Xandy(df, label)\n",
    "    X = dummy_code(X.copy())\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=5)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k)\n",
    "    if random==True: random_state = 0\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "    model_rfc = RandomForestClassifier(random_state=random_state)\n",
    "    model_ridge = RidgeClassifier(random_state=random_state)\n",
    "    model_gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "    model_log = LogisticRegression(random_state=random_state, max_iter=10000)\n",
    "    scores_rfc = cross_val_score(model_rfc, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_ridge = cross_val_score(model_ridge, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_gbc = cross_val_score(model_gbc, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_log = cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    if messages == True:\n",
    "        print(f'Accuracy (RandomForest):\\t{np.mean(scores_rfc)}')\n",
    "        print(f'Accuracy (Ridge):\\t{np.mean(scores_ridge)}')\n",
    "        print(f'Accuracy (GradientBoosting):\\t{np.mean(scores_gbc)}')\n",
    "        print(f'Accuracy (Log):\\t{np.mean(scores_log)}')\n",
    "    scores = {np.mean(scores_rfc):model_rfc, \n",
    "            np.mean(scores_gbc):model_gbc, \n",
    "            np.mean(scores_ridge):model_ridge, \n",
    "            np.mean(scores_log):model_log }    \n",
    "    return scores[max(scores.keys())].fit(X, y)\n",
    "\n",
    "def select_features(df, label, model, messages=True):\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "  import pandas as pd\n",
    "\n",
    "  y = df[label]\n",
    "  X = df.drop(columns=[label])\n",
    "  X = pd.get_dummies(X.copy(), drop_first=True)\n",
    "\n",
    "  sel = SelectFromModel(model, prefit=True)\n",
    "  sel.transform(X)\n",
    "\n",
    "  columns = list(X.columns[sel.get_support()])\n",
    "  new_df = X[columns]\n",
    "  new_df[label] = y\n",
    "  return new_df\n",
    "\n",
    "def fit_cv_model_expanded(df, label, k=10, r=5, repeat=True, random_state=0):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Assuming Xandy is a function to split features (X) and target (y)\n",
    "    X, y = Xandy(df, label)\n",
    "\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    fit = {}    # Use this to store each of the fit metrics\n",
    "    models = {} # Use this to store each of the models\n",
    "\n",
    "    # Create the model objects\n",
    "    model_logistic = LogisticRegression(random_state=random_state)\n",
    "    model_rf = RandomForestClassifier(random_state=random_state)\n",
    "    model_gb = GradientBoostingClassifier(random_state=random_state)\n",
    "    model_ab = AdaBoostClassifier(n_estimators=100, random_state=random_state)\n",
    "    model_svc = SVC(random_state=random_state)\n",
    "    model_lsvc = LinearSVC(random_state=random_state)\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    model_xgb = XGBClassifier(n_estimators=1000, max_depth=7, learning_rate=0.1, subsample=0.7, colsample_bytree=0.8, random_state=random_state)\n",
    "    model_mlp = MLPClassifier(max_iter=1000, random_state=random_state)\n",
    "\n",
    "    # Fit a cross-validated accuracy score and add it to the dict\n",
    "    fit['Logistic Regression'] = np.mean(cross_val_score(model_logistic, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Random Forest'] = np.mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Gradient Boosting'] = np.mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['AdaBoost'] = np.mean(cross_val_score(model_ab, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['SVM'] = np.mean(cross_val_score(model_svc, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Linear SVM'] = np.mean(cross_val_score(model_lsvc, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['KNN'] = np.mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['XGBoost'] = np.mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Neural Network'] = np.mean(cross_val_score(model_mlp, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "    # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "    models['Logistic Regression'] = model_logistic\n",
    "    models['Random Forest'] = model_rf\n",
    "    models['Gradient Boosting'] = model_gb\n",
    "    models['AdaBoost'] = model_ab\n",
    "    models['SVM'] = model_svc\n",
    "    models['Linear SVM'] = model_lsvc\n",
    "    models['KNN'] = model_knn\n",
    "    models['XGBoost'] = model_xgb\n",
    "    models['Neural Network'] = model_mlp\n",
    "\n",
    "    # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "    df_fit = pd.DataFrame({'Accuracy': fit})\n",
    "    df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "    print(df_fit)\n",
    "    best_model = df_fit.index[0]\n",
    "    \n",
    "\n",
    "    return models[best_model].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are not limiting down to CVC and Online only because if there is other types of transactions those need to be known\n",
    "new_5_percent_df = bin_categories(df.copy(), cutoff=0.05)\n",
    "label = 'Fraud'\n",
    "new_5_percent_df = missing_data(new_5_percent_df, label)\n",
    "model = fit_cv_model(new_5_percent_df, k=5, label=label, messages=False)\n",
    "df_reduced = select_features(new_5_percent_df.copy(), label=label, model=model, messages=False)\n",
    "model = fit_cv_model_expanded(df_reduced, k=5, label=label)\n",
    "X, y = Xandy(df_reduced, label)\n",
    "print(X.shape)\n",
    "print(df_reduced.shape)\n",
    "for col in X:\n",
    "    print(f'{col}')\n",
    "for col in df_reduced:\n",
    "    print(f'{col}')\n",
    "initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "with open(\"wwwroot/fraud_onnx_model.onnx\", \"wb\")as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
