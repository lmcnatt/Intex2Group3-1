{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gooch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\gooch\\AppData\\Local\\Temp\\ipykernel_15768\\1602739384.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[label] = y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy\n",
      "Random Forest          0.9626\n",
      "Gradient Boosting      0.9618\n",
      "XGBoost                0.9562\n",
      "AdaBoost               0.9542\n",
      "Logistic Regression    0.9510\n",
      "SVM                    0.9380\n",
      "KNN                    0.9342\n",
      "Linear SVM             0.9004\n",
      "Neural Network         0.7956\n",
      "RandomForestClassifier(random_state=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import onnx # not used but can be used to inspect and manipulate saved onnx model\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "conn = sqlite3.connect('intex2.db')\n",
    "cursor = conn.cursor()\n",
    "read_sql = \"SELECT * from Orders\"\n",
    "df = pd.read_sql_query(read_sql, conn)\n",
    "df\n",
    "\n",
    "\n",
    "#This has all the functions\n",
    "def bin_categories(df, features=[], cutoff=0.05, replace_with='Other', messages=False):\n",
    "\n",
    "  import pandas as pd\n",
    "\n",
    "  if len(features) == 0: features = df.columns\n",
    "\n",
    "  for feat in features:\n",
    "    if feat in df.columns:\n",
    "      if not pd.api.types.is_numeric_dtype(df[feat]):\n",
    "        other_list = df[feat].value_counts()[df[feat].value_counts() / df.shape[0] < cutoff].index\n",
    "        df.loc[df[feat].isin(other_list), feat] = replace_with\n",
    "        if messages: print(f'{feat} has been binned by setting {other_list} to {replace_with}')\n",
    "    else:\n",
    "      if messages: print(f'{feat} not found in the DataFrame provided. No binning performed')\n",
    "\n",
    "  return df\n",
    "\n",
    "def Xandy(df, label):\n",
    "    import pandas as pd\n",
    "    y = df[label]\n",
    "    X = df.drop(columns=[label])\n",
    "    return X, y\n",
    "\n",
    "def dummy_code(X):\n",
    "    import pandas as pd\n",
    "    X = pd.get_dummies(X.copy(), drop_first=True)\n",
    "    return X\n",
    "\n",
    "def missing_data(df, label, row_thresh = 0.7, col_thresh = 0.9, random=False, random_state=3):\n",
    "    import pandas as pd\n",
    "    df.dropna(axis='rows', subset=[label], inplace=True)\n",
    "    df.dropna(axis='columns', thresh=1, inplace=True)\n",
    "    df.dropna(axis='rows', thresh=1, inplace=True)\n",
    "    df.dropna(axis='columns', thresh=round(df.shape[0] * row_thresh), inplace=True)\n",
    "    df.dropna(axis='rows', thresh=round(df.shape[1] * col_thresh), inplace=True)\n",
    "    #impute values\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer, KNNImputer\n",
    "        X, y = Xandy(df, label)\n",
    "        X = dummy_code(X.copy())\n",
    "        if random: random_state = 0\n",
    "        imp = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "        X = pd.DataFrame(imp.fit_transform(X), columns = X.columns, index = X.index)\n",
    "        df = X.merge(y, left_index=True, right_index=True)\n",
    "    else:\n",
    "        X, y = Xandy(df, label)\n",
    "        X = dummy_code(X.copy())\n",
    "        df = X.merge(y, left_index=True, right_index=True)\n",
    "    return df\n",
    "\n",
    "def fit_cv_model(df, label, k=5, repeat=True, random=False, random_state=3, messages=False):\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    X, y = Xandy(df, label)\n",
    "    X = dummy_code(X.copy())\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=5)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k)\n",
    "    if random==True: random_state = 0\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "    model_rfc = RandomForestClassifier(random_state=random_state)\n",
    "    model_ridge = RidgeClassifier(random_state=random_state)\n",
    "    model_gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "    model_log = LogisticRegression(random_state=random_state, max_iter=10000)\n",
    "    scores_rfc = cross_val_score(model_rfc, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_ridge = cross_val_score(model_ridge, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_gbc = cross_val_score(model_gbc, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores_log = cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    if messages == True:\n",
    "        print(f'Accuracy (RandomForest):\\t{np.mean(scores_rfc)}')\n",
    "        print(f'Accuracy (Ridge):\\t{np.mean(scores_ridge)}')\n",
    "        print(f'Accuracy (GradientBoosting):\\t{np.mean(scores_gbc)}')\n",
    "        print(f'Accuracy (Log):\\t{np.mean(scores_log)}')\n",
    "    scores = {np.mean(scores_rfc):model_rfc, \n",
    "            np.mean(scores_gbc):model_gbc, \n",
    "            np.mean(scores_ridge):model_ridge, \n",
    "            np.mean(scores_log):model_log }    \n",
    "    return scores[max(scores.keys())].fit(X, y)\n",
    "\n",
    "def select_features(df, label, model, messages=True):\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "  import pandas as pd\n",
    "\n",
    "  y = df[label]\n",
    "  X = df.drop(columns=[label])\n",
    "  X = pd.get_dummies(X.copy(), drop_first=True)\n",
    "\n",
    "  sel = SelectFromModel(model, prefit=True)\n",
    "  sel.transform(X)\n",
    "\n",
    "  columns = list(X.columns[sel.get_support()])\n",
    "  new_df = X[columns]\n",
    "  new_df[label] = y\n",
    "  return new_df\n",
    "\n",
    "def fit_cv_model_expanded(df, label, k=10, r=5, repeat=True, random_state=0):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Assuming Xandy is a function to split features (X) and target (y)\n",
    "    X, y = Xandy(df, label)\n",
    "\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    fit = {}    # Use this to store each of the fit metrics\n",
    "    models = {} # Use this to store each of the models\n",
    "\n",
    "    # Create the model objects\n",
    "    model_logistic = LogisticRegression(random_state=random_state)\n",
    "    model_rf = RandomForestClassifier(random_state=random_state)\n",
    "    model_gb = GradientBoostingClassifier(random_state=random_state)\n",
    "    model_ab = AdaBoostClassifier(n_estimators=100, random_state=random_state)\n",
    "    model_svc = SVC(random_state=random_state)\n",
    "    model_lsvc = LinearSVC(random_state=random_state)\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    model_xgb = XGBClassifier(n_estimators=1000, max_depth=7, learning_rate=0.1, subsample=0.7, colsample_bytree=0.8, random_state=random_state)\n",
    "    model_mlp = MLPClassifier(max_iter=1000, random_state=random_state)\n",
    "\n",
    "    # Fit a cross-validated accuracy score and add it to the dict\n",
    "    fit['Logistic Regression'] = np.mean(cross_val_score(model_logistic, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Random Forest'] = np.mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Gradient Boosting'] = np.mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['AdaBoost'] = np.mean(cross_val_score(model_ab, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['SVM'] = np.mean(cross_val_score(model_svc, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Linear SVM'] = np.mean(cross_val_score(model_lsvc, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['KNN'] = np.mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['XGBoost'] = np.mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Neural Network'] = np.mean(cross_val_score(model_mlp, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "    # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "    models['Logistic Regression'] = model_logistic\n",
    "    models['Random Forest'] = model_rf\n",
    "    models['Gradient Boosting'] = model_gb\n",
    "    models['AdaBoost'] = model_ab\n",
    "    models['SVM'] = model_svc\n",
    "    models['Linear SVM'] = model_lsvc\n",
    "    models['KNN'] = model_knn\n",
    "    models['XGBoost'] = model_xgb\n",
    "    models['Neural Network'] = model_mlp\n",
    "\n",
    "    # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "    df_fit = pd.DataFrame({'Accuracy': fit})\n",
    "    df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "    best_model = df_fit.index[0]\n",
    "    \n",
    "\n",
    "    return models[best_model].fit(X, y)\n",
    "\n",
    "\n",
    "df = df.sample(n=1000)\n",
    "#We are not limiting down to CVC and Online only because if there is other types of transactions those need to be known\n",
    "new_5_percent_df = bin_categories(df.copy(), cutoff=0.05)\n",
    "label = 'Fraud'\n",
    "new_5_percent_df = missing_data(new_5_percent_df, label)\n",
    "model = fit_cv_model(new_5_percent_df, k=5, label=label, messages=False)\n",
    "df_reduced = select_features(new_5_percent_df.copy(), label=label, model=model, messages=False)\n",
    "model = fit_cv_model_expanded(df_reduced, k=5, label=label)\n",
    "X, y = Xandy(df, label)\n",
    "initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "\n",
    "with open(\"fraud_onnx_model.onnx\", \"wb\")as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
